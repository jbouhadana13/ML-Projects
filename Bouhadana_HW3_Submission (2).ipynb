{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04abab0d",
   "metadata": {},
   "source": [
    "# 1. Problem 1: Linear Regression Model (38 points)\n",
    "In this problem, we want to use some statistical techniques we learned to predict values for continuous variables using Linear Regression model that we build from scratch. Download the zip file for\n",
    "Homework 3, and use AMZN dataset which contains historical daily prices for all tickers currently\n",
    "trading on NASDAQ. The up to date list is available from nasdaqtrader.com. The historic data is\n",
    "retrieved from Yahoo finance via yfinance python package.\n",
    "\n",
    "## (a) Try to predict the closing price of AMZN stock based on last 10 days of closing prices.\n",
    "    • Use both Normal Equation Method and Gradient Descent Method.\n",
    "    • See if you get the same answer using both methods.\n",
    "    • See how your answers evolve with the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64a8b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages to the jupyter notebook\n",
    "# Implement a Linear Regression model using both Normal Equation Method and SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read and load the csv data file\n",
    "filename = \"/Users/jeremybouhadana/Downloads/hw03-DataSet/AMZN.csv\"\n",
    "data = read_csv( filename )\n",
    "\n",
    "# Get the Adjusted Close Price\n",
    "data_select = data[['Adj Close']]\n",
    "\n",
    "# converting the dataset to a numpy array\n",
    "values = data_select.values\n",
    "\n",
    "# get the number of rows in the dataset\n",
    "n_rows = len(data_select.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50bbff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var 1(t-10)   var 1(t-9)   var 1(t-8)   var 1(t-7)   var 1(t-6)  \\\n",
      "10       1.958333     1.729167     1.708333     1.635417     1.427083   \n",
      "11       1.729167     1.708333     1.635417     1.427083     1.395833   \n",
      "12       1.708333     1.635417     1.427083     1.395833     1.500000   \n",
      "13       1.635417     1.427083     1.395833     1.500000     1.583333   \n",
      "14       1.427083     1.395833     1.500000     1.583333     1.531250   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "5753  1676.609985  1785.000000  1689.150024  1807.839966  1830.000000   \n",
      "5754  1785.000000  1689.150024  1807.839966  1830.000000  1880.930054   \n",
      "5755  1689.150024  1807.839966  1830.000000  1880.930054  1846.089966   \n",
      "5756  1807.839966  1830.000000  1880.930054  1846.089966  1902.829956   \n",
      "5757  1830.000000  1880.930054  1846.089966  1902.829956  1940.099976   \n",
      "\n",
      "       var 1(t-5)   var 1(t-4)   var 1(t-3)   var 1(t-2)   var 1(t-1)  \\\n",
      "10       1.395833     1.500000     1.583333     1.531250     1.505208   \n",
      "11       1.500000     1.583333     1.531250     1.505208     1.500000   \n",
      "12       1.583333     1.531250     1.505208     1.500000     1.510417   \n",
      "13       1.531250     1.505208     1.500000     1.510417     1.479167   \n",
      "14       1.505208     1.500000     1.510417     1.479167     1.416667   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "5753  1880.930054  1846.089966  1902.829956  1940.099976  1885.839966   \n",
      "5754  1846.089966  1902.829956  1940.099976  1885.839966  1955.489990   \n",
      "5755  1902.829956  1940.099976  1885.839966  1955.489990  1900.099976   \n",
      "5756  1940.099976  1885.839966  1955.489990  1900.099976  1963.949951   \n",
      "5757  1885.839966  1955.489990  1900.099976  1963.949951  1949.719971   \n",
      "\n",
      "          var1(t)  \n",
      "10       1.500000  \n",
      "11       1.510417  \n",
      "12       1.479167  \n",
      "13       1.416667  \n",
      "14       1.541667  \n",
      "...           ...  \n",
      "5753  1955.489990  \n",
      "5754  1900.099976  \n",
      "5755  1963.949951  \n",
      "5756  1949.719971  \n",
      "5757  1907.699951  \n",
      "\n",
      "[5748 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#series to supervised \n",
    "\n",
    "def series_to_supervised (data , n_in =10, n_out =1, dropnan = True ):\n",
    "\n",
    "  \"\"\"\n",
    "Arguments :\n",
    "  data : Sequence of observations as a list or NumPy array .\n",
    "  n_in : Number of lag observations as input (X).\n",
    "  n_out : Number of observations as output (y).\n",
    "  dropnan : Boolean whether or not to drop rows with NaN values .\n",
    "Returns :\n",
    "  Pandas DataFrame of series framed for supervised learning .\n",
    "  \"\"\"\n",
    "\n",
    "  n_vars = 1 if type ( data ) is list else data . shape [1]\n",
    "  df = DataFrame(data)\n",
    "  cols , names = list () , list ()\n",
    "\n",
    "# input sequence (t-n, ... t-1)\n",
    "  for i in range (n_in , 0, -1):\n",
    "    cols . append (df. shift (i))\n",
    "    names += [('var %d(t-%d)' % (j+1, i)) for j in range ( n_vars )]\n",
    "\n",
    "# forecast sequence (t, t+1, ... t+n)\n",
    "  for i in range (0, n_out ):\n",
    "    cols . append (df. shift (-i))\n",
    "    if i == 0:\n",
    "      names += [('var%d(t)' % (j+1)) for j in range ( n_vars )]\n",
    "    else :\n",
    "      names += [('var%d(t+%d)' % (j+1, i)) for j in range ( n_vars )]\n",
    "\n",
    "# put it all together\n",
    "  agg = concat (cols , axis =1)\n",
    "  agg . columns = names\n",
    "\n",
    "# drop rows with NaN values\n",
    "  if dropnan :\n",
    "    agg. dropna ( inplace = True )\n",
    "  return agg\n",
    "#Output\n",
    "\n",
    "new_values = series_to_supervised(values, 10)\n",
    "print(new_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff92d0",
   "metadata": {},
   "source": [
    "Use the Python function (given at the end of the document) named series to supervised()\n",
    "that takes a univariate or multivariate time series and frames it as a supervised learning dataset\n",
    "(10 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc122b64",
   "metadata": {},
   "source": [
    "## (b) Use MinMaxScaler to scale your data. (2 points)\n",
    "Hint: Before this minmax normalization, don’t forgot to convert your new\n",
    "transformed dataset (that you created in previous step) into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dcee5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var 1(t-10)  var 1(t-9)  var 1(t-8)  var 1(t-7)  var 1(t-6)  var 1(t-5)  \\\n",
      "10       0.000259    0.000154    0.000144    0.000110    0.000014    0.000000   \n",
      "11       0.000154    0.000144    0.000110    0.000014    0.000000    0.000048   \n",
      "12       0.000144    0.000110    0.000014    0.000000    0.000048    0.000086   \n",
      "13       0.000110    0.000014    0.000000    0.000048    0.000086    0.000062   \n",
      "14       0.000014    0.000000    0.000048    0.000086    0.000062    0.000050   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "5753     0.772407    0.822383    0.778189    0.832914    0.843132    0.866614   \n",
      "5754     0.822383    0.778189    0.832914    0.843132    0.866614    0.850550   \n",
      "5755     0.778189    0.832914    0.843132    0.866614    0.850550    0.876712   \n",
      "5756     0.832914    0.843132    0.866614    0.850550    0.876712    0.893896   \n",
      "5757     0.843132    0.866614    0.850550    0.876712    0.893896    0.868878   \n",
      "\n",
      "      var 1(t-4)  var 1(t-3)  var 1(t-2)  var 1(t-1)   var1(t)  \n",
      "10      0.000048    0.000086    0.000062    0.000050  0.000048  \n",
      "11      0.000086    0.000062    0.000050    0.000048  0.000053  \n",
      "12      0.000062    0.000050    0.000048    0.000053  0.000038  \n",
      "13      0.000050    0.000048    0.000053    0.000038  0.000010  \n",
      "14      0.000048    0.000053    0.000038    0.000010  0.000067  \n",
      "...          ...         ...         ...         ...       ...  \n",
      "5753    0.850550    0.876712    0.893896    0.868878  0.900992  \n",
      "5754    0.876712    0.893896    0.868878    0.900992  0.875453  \n",
      "5755    0.893896    0.868878    0.900992    0.875453  0.904893  \n",
      "5756    0.868878    0.900992    0.875453    0.904893  0.898332  \n",
      "5757    0.900992    0.875453    0.904893    0.898332  0.878957  \n",
      "\n",
      "[5748 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(data_select)\n",
    "\n",
    "# Convert the dataset to a numpy array\n",
    "values = series_to_supervised(scaled, n_in=10, n_out=1, dropnan=True)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcf001",
   "metadata": {},
   "source": [
    "## (c) Use the Normal Equation Method to find the linear regression coefficients (w). \n",
    "To perform this you may want to take the following steps first: Split your data to X and Y by taking the\n",
    "columns var1(t-10),...,var(t-1) as your 10 features in X, and take the last column var1(t) as your\n",
    "target (Y). Expand your matrix X with a bias vector of ones as the first column (to accomplish\n",
    "this, you may want to use the numpy operations np.ones , np.reshape and np.append ).\n",
    "Use the train test split with ‘random state=1’ to split your data to 70% training, and\n",
    "30% test data. Solve the Normal Equation Method in (2) to find the coefficients w. (10 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9e030c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15951918]\n"
     ]
    }
   ],
   "source": [
    "# split into X and Y\n",
    "X = scaled[:, :-1]\n",
    "Y = scaled[:, -1]\n",
    "\n",
    "# expand X with a bias vector of ones\n",
    "X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "\n",
    "# split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n",
    "\n",
    "# use the Normal Equation Method to find the coefficients w\n",
    "w = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f548d5",
   "metadata": {},
   "source": [
    "## (d) Make a prediction on your test set using the linear regression function f(x) = wT x\n",
    " and use both the mean square error and coefficient of determination R2\n",
    "to measure the performance\n",
    "of your prediction model. For this use fucntions mean squared error and r2 score from\n",
    "sklearn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7976881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  0.055891283619644405\n",
      "R2 Score:  -0.0020425929959138056\n"
     ]
    }
   ],
   "source": [
    "# make predictions on test set using the linear regression function\n",
    "Y_pred = np.dot(X_test, w)\n",
    "\n",
    "# calculate mean squared error and coefficient of determination R2\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# print the results\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"R2 Score: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3ecdf",
   "metadata": {},
   "source": [
    "## (e) Next, find the coefficients w using gradient descent algorithm and monitor how your error changes in each epoch\n",
    "You can create a function coefficients sgd similar to what we did in our Lab Session 7. Note that you may have to make some minor changes to this part of the code ( coefficients sgd for linear regression, in lab session 7), due to the additional bias term 1 in your matrix X. For this part, use learning rate 0.01, and number of epochs(iterations) 200. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a2d4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0] # is bias\n",
    "    for i in range(len(row)):\n",
    "        yhat = yhat + coefficients[i + 1] * row[i] # b+ W * x(inputs - row)\n",
    "    return yhat\n",
    "    \n",
    "def coefficients_sgd(X_train, Y_train, l_rate, n_epoch): #l-rate is learning rate\n",
    "  #initializing all coefficients to zero\n",
    "    coef = [0.0 for i in range(len(X_train[0])+1)]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0 # loss\n",
    "        for i in range(X_train.shape[0]):\n",
    "            # calculating the prediction using current coeeficients\n",
    "            yhat = predict(X_train[i,:], coef)\n",
    "            # calculating error\n",
    "            error = yhat - Y_train[i] #yhat is prediction, Y_train is ground truth,\n",
    "            sum_error += error**2 # error square, because loss cannot be negative, or we want to error to be positive.\n",
    "            #stochastic gradient descent\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for j in range(len(coef)-1):\n",
    "                coef[j + 1] = coef[j + 1] - l_rate * error * X_train[i,j]\n",
    "    \n",
    "        print( ' >epoch=%d, lrate=%.3f, error=%.3f ' % (epoch, l_rate, sum_error))\n",
    "    #returning the list of coefficients  \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "694bcadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >epoch=0, lrate=0.010, error=241.889 \n",
      " >epoch=1, lrate=0.010, error=241.072 \n",
      " >epoch=2, lrate=0.010, error=241.072 \n",
      " >epoch=3, lrate=0.010, error=241.072 \n",
      " >epoch=4, lrate=0.010, error=241.072 \n",
      " >epoch=5, lrate=0.010, error=241.072 \n",
      " >epoch=6, lrate=0.010, error=241.072 \n",
      " >epoch=7, lrate=0.010, error=241.072 \n",
      " >epoch=8, lrate=0.010, error=241.072 \n",
      " >epoch=9, lrate=0.010, error=241.072 \n",
      " >epoch=10, lrate=0.010, error=241.072 \n",
      " >epoch=11, lrate=0.010, error=241.072 \n",
      " >epoch=12, lrate=0.010, error=241.072 \n",
      " >epoch=13, lrate=0.010, error=241.072 \n",
      " >epoch=14, lrate=0.010, error=241.072 \n",
      " >epoch=15, lrate=0.010, error=241.072 \n",
      " >epoch=16, lrate=0.010, error=241.072 \n",
      " >epoch=17, lrate=0.010, error=241.072 \n",
      " >epoch=18, lrate=0.010, error=241.072 \n",
      " >epoch=19, lrate=0.010, error=241.072 \n",
      " >epoch=20, lrate=0.010, error=241.072 \n",
      " >epoch=21, lrate=0.010, error=241.072 \n",
      " >epoch=22, lrate=0.010, error=241.072 \n",
      " >epoch=23, lrate=0.010, error=241.072 \n",
      " >epoch=24, lrate=0.010, error=241.072 \n",
      " >epoch=25, lrate=0.010, error=241.072 \n",
      " >epoch=26, lrate=0.010, error=241.072 \n",
      " >epoch=27, lrate=0.010, error=241.072 \n",
      " >epoch=28, lrate=0.010, error=241.072 \n",
      " >epoch=29, lrate=0.010, error=241.072 \n",
      " >epoch=30, lrate=0.010, error=241.072 \n",
      " >epoch=31, lrate=0.010, error=241.072 \n",
      " >epoch=32, lrate=0.010, error=241.072 \n",
      " >epoch=33, lrate=0.010, error=241.072 \n",
      " >epoch=34, lrate=0.010, error=241.072 \n",
      " >epoch=35, lrate=0.010, error=241.072 \n",
      " >epoch=36, lrate=0.010, error=241.072 \n",
      " >epoch=37, lrate=0.010, error=241.072 \n",
      " >epoch=38, lrate=0.010, error=241.072 \n",
      " >epoch=39, lrate=0.010, error=241.072 \n",
      " >epoch=40, lrate=0.010, error=241.072 \n",
      " >epoch=41, lrate=0.010, error=241.072 \n",
      " >epoch=42, lrate=0.010, error=241.072 \n",
      " >epoch=43, lrate=0.010, error=241.072 \n",
      " >epoch=44, lrate=0.010, error=241.072 \n",
      " >epoch=45, lrate=0.010, error=241.072 \n",
      " >epoch=46, lrate=0.010, error=241.072 \n",
      " >epoch=47, lrate=0.010, error=241.072 \n",
      " >epoch=48, lrate=0.010, error=241.072 \n",
      " >epoch=49, lrate=0.010, error=241.072 \n",
      " >epoch=50, lrate=0.010, error=241.072 \n",
      " >epoch=51, lrate=0.010, error=241.072 \n",
      " >epoch=52, lrate=0.010, error=241.072 \n",
      " >epoch=53, lrate=0.010, error=241.072 \n",
      " >epoch=54, lrate=0.010, error=241.072 \n",
      " >epoch=55, lrate=0.010, error=241.072 \n",
      " >epoch=56, lrate=0.010, error=241.072 \n",
      " >epoch=57, lrate=0.010, error=241.072 \n",
      " >epoch=58, lrate=0.010, error=241.072 \n",
      " >epoch=59, lrate=0.010, error=241.072 \n",
      " >epoch=60, lrate=0.010, error=241.072 \n",
      " >epoch=61, lrate=0.010, error=241.072 \n",
      " >epoch=62, lrate=0.010, error=241.072 \n",
      " >epoch=63, lrate=0.010, error=241.072 \n",
      " >epoch=64, lrate=0.010, error=241.072 \n",
      " >epoch=65, lrate=0.010, error=241.072 \n",
      " >epoch=66, lrate=0.010, error=241.072 \n",
      " >epoch=67, lrate=0.010, error=241.072 \n",
      " >epoch=68, lrate=0.010, error=241.072 \n",
      " >epoch=69, lrate=0.010, error=241.072 \n",
      " >epoch=70, lrate=0.010, error=241.072 \n",
      " >epoch=71, lrate=0.010, error=241.072 \n",
      " >epoch=72, lrate=0.010, error=241.072 \n",
      " >epoch=73, lrate=0.010, error=241.072 \n",
      " >epoch=74, lrate=0.010, error=241.072 \n",
      " >epoch=75, lrate=0.010, error=241.072 \n",
      " >epoch=76, lrate=0.010, error=241.072 \n",
      " >epoch=77, lrate=0.010, error=241.072 \n",
      " >epoch=78, lrate=0.010, error=241.072 \n",
      " >epoch=79, lrate=0.010, error=241.072 \n",
      " >epoch=80, lrate=0.010, error=241.072 \n",
      " >epoch=81, lrate=0.010, error=241.072 \n",
      " >epoch=82, lrate=0.010, error=241.072 \n",
      " >epoch=83, lrate=0.010, error=241.072 \n",
      " >epoch=84, lrate=0.010, error=241.072 \n",
      " >epoch=85, lrate=0.010, error=241.072 \n",
      " >epoch=86, lrate=0.010, error=241.072 \n",
      " >epoch=87, lrate=0.010, error=241.072 \n",
      " >epoch=88, lrate=0.010, error=241.072 \n",
      " >epoch=89, lrate=0.010, error=241.072 \n",
      " >epoch=90, lrate=0.010, error=241.072 \n",
      " >epoch=91, lrate=0.010, error=241.072 \n",
      " >epoch=92, lrate=0.010, error=241.072 \n",
      " >epoch=93, lrate=0.010, error=241.072 \n",
      " >epoch=94, lrate=0.010, error=241.072 \n",
      " >epoch=95, lrate=0.010, error=241.072 \n",
      " >epoch=96, lrate=0.010, error=241.072 \n",
      " >epoch=97, lrate=0.010, error=241.072 \n",
      " >epoch=98, lrate=0.010, error=241.072 \n",
      " >epoch=99, lrate=0.010, error=241.072 \n",
      " >epoch=100, lrate=0.010, error=241.072 \n",
      " >epoch=101, lrate=0.010, error=241.072 \n",
      " >epoch=102, lrate=0.010, error=241.072 \n",
      " >epoch=103, lrate=0.010, error=241.072 \n",
      " >epoch=104, lrate=0.010, error=241.072 \n",
      " >epoch=105, lrate=0.010, error=241.072 \n",
      " >epoch=106, lrate=0.010, error=241.072 \n",
      " >epoch=107, lrate=0.010, error=241.072 \n",
      " >epoch=108, lrate=0.010, error=241.072 \n",
      " >epoch=109, lrate=0.010, error=241.072 \n",
      " >epoch=110, lrate=0.010, error=241.072 \n",
      " >epoch=111, lrate=0.010, error=241.072 \n",
      " >epoch=112, lrate=0.010, error=241.072 \n",
      " >epoch=113, lrate=0.010, error=241.072 \n",
      " >epoch=114, lrate=0.010, error=241.072 \n",
      " >epoch=115, lrate=0.010, error=241.072 \n",
      " >epoch=116, lrate=0.010, error=241.072 \n",
      " >epoch=117, lrate=0.010, error=241.072 \n",
      " >epoch=118, lrate=0.010, error=241.072 \n",
      " >epoch=119, lrate=0.010, error=241.072 \n",
      " >epoch=120, lrate=0.010, error=241.072 \n",
      " >epoch=121, lrate=0.010, error=241.072 \n",
      " >epoch=122, lrate=0.010, error=241.072 \n",
      " >epoch=123, lrate=0.010, error=241.072 \n",
      " >epoch=124, lrate=0.010, error=241.072 \n",
      " >epoch=125, lrate=0.010, error=241.072 \n",
      " >epoch=126, lrate=0.010, error=241.072 \n",
      " >epoch=127, lrate=0.010, error=241.072 \n",
      " >epoch=128, lrate=0.010, error=241.072 \n",
      " >epoch=129, lrate=0.010, error=241.072 \n",
      " >epoch=130, lrate=0.010, error=241.072 \n",
      " >epoch=131, lrate=0.010, error=241.072 \n",
      " >epoch=132, lrate=0.010, error=241.072 \n",
      " >epoch=133, lrate=0.010, error=241.072 \n",
      " >epoch=134, lrate=0.010, error=241.072 \n",
      " >epoch=135, lrate=0.010, error=241.072 \n",
      " >epoch=136, lrate=0.010, error=241.072 \n",
      " >epoch=137, lrate=0.010, error=241.072 \n",
      " >epoch=138, lrate=0.010, error=241.072 \n",
      " >epoch=139, lrate=0.010, error=241.072 \n",
      " >epoch=140, lrate=0.010, error=241.072 \n",
      " >epoch=141, lrate=0.010, error=241.072 \n",
      " >epoch=142, lrate=0.010, error=241.072 \n",
      " >epoch=143, lrate=0.010, error=241.072 \n",
      " >epoch=144, lrate=0.010, error=241.072 \n",
      " >epoch=145, lrate=0.010, error=241.072 \n",
      " >epoch=146, lrate=0.010, error=241.072 \n",
      " >epoch=147, lrate=0.010, error=241.072 \n",
      " >epoch=148, lrate=0.010, error=241.072 \n",
      " >epoch=149, lrate=0.010, error=241.072 \n",
      " >epoch=150, lrate=0.010, error=241.072 \n",
      " >epoch=151, lrate=0.010, error=241.072 \n",
      " >epoch=152, lrate=0.010, error=241.072 \n",
      " >epoch=153, lrate=0.010, error=241.072 \n",
      " >epoch=154, lrate=0.010, error=241.072 \n",
      " >epoch=155, lrate=0.010, error=241.072 \n",
      " >epoch=156, lrate=0.010, error=241.072 \n",
      " >epoch=157, lrate=0.010, error=241.072 \n",
      " >epoch=158, lrate=0.010, error=241.072 \n",
      " >epoch=159, lrate=0.010, error=241.072 \n",
      " >epoch=160, lrate=0.010, error=241.072 \n",
      " >epoch=161, lrate=0.010, error=241.072 \n",
      " >epoch=162, lrate=0.010, error=241.072 \n",
      " >epoch=163, lrate=0.010, error=241.072 \n",
      " >epoch=164, lrate=0.010, error=241.072 \n",
      " >epoch=165, lrate=0.010, error=241.072 \n",
      " >epoch=166, lrate=0.010, error=241.072 \n",
      " >epoch=167, lrate=0.010, error=241.072 \n",
      " >epoch=168, lrate=0.010, error=241.072 \n",
      " >epoch=169, lrate=0.010, error=241.072 \n",
      " >epoch=170, lrate=0.010, error=241.072 \n",
      " >epoch=171, lrate=0.010, error=241.072 \n",
      " >epoch=172, lrate=0.010, error=241.072 \n",
      " >epoch=173, lrate=0.010, error=241.072 \n",
      " >epoch=174, lrate=0.010, error=241.072 \n",
      " >epoch=175, lrate=0.010, error=241.072 \n",
      " >epoch=176, lrate=0.010, error=241.072 \n",
      " >epoch=177, lrate=0.010, error=241.072 \n",
      " >epoch=178, lrate=0.010, error=241.072 \n",
      " >epoch=179, lrate=0.010, error=241.072 \n",
      " >epoch=180, lrate=0.010, error=241.072 \n",
      " >epoch=181, lrate=0.010, error=241.072 \n",
      " >epoch=182, lrate=0.010, error=241.072 \n",
      " >epoch=183, lrate=0.010, error=241.072 \n",
      " >epoch=184, lrate=0.010, error=241.072 \n",
      " >epoch=185, lrate=0.010, error=241.072 \n",
      " >epoch=186, lrate=0.010, error=241.072 \n",
      " >epoch=187, lrate=0.010, error=241.072 \n",
      " >epoch=188, lrate=0.010, error=241.072 \n",
      " >epoch=189, lrate=0.010, error=241.072 \n",
      " >epoch=190, lrate=0.010, error=241.072 \n",
      " >epoch=191, lrate=0.010, error=241.072 \n",
      " >epoch=192, lrate=0.010, error=241.072 \n",
      " >epoch=193, lrate=0.010, error=241.072 \n",
      " >epoch=194, lrate=0.010, error=241.072 \n",
      " >epoch=195, lrate=0.010, error=241.072 \n",
      " >epoch=196, lrate=0.010, error=241.072 \n",
      " >epoch=197, lrate=0.010, error=241.072 \n",
      " >epoch=198, lrate=0.010, error=241.072 \n",
      " >epoch=199, lrate=0.010, error=241.072 \n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.01\n",
    "n_epoch = 200\n",
    "\n",
    "coef = coefficients_sgd(X_train, Y_train, l_rate, n_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527a753",
   "metadata": {},
   "source": [
    "## (f) Make a prediction using the coefficients you found from SGD algorithm in previous step\n",
    "(Y prediction sgd = X test.dot(coef sgd)); Use both the mean square error and coefficient of determination R2\n",
    "to measure the performance of your predictions; compare the results\n",
    "with your prediction performance in part d where you used the coefficients found from Normal\n",
    "Equation Method. Which method gives you better results? (6 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f30e0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >epoch=0, lrate=0.010, error=243.031 \n",
      " >epoch=1, lrate=0.010, error=242.457 \n",
      " >epoch=2, lrate=0.010, error=242.457 \n",
      " >epoch=3, lrate=0.010, error=242.457 \n",
      " >epoch=4, lrate=0.010, error=242.457 \n",
      " >epoch=5, lrate=0.010, error=242.457 \n",
      " >epoch=6, lrate=0.010, error=242.457 \n",
      " >epoch=7, lrate=0.010, error=242.457 \n",
      " >epoch=8, lrate=0.010, error=242.457 \n",
      " >epoch=9, lrate=0.010, error=242.457 \n",
      " >epoch=10, lrate=0.010, error=242.457 \n",
      " >epoch=11, lrate=0.010, error=242.457 \n",
      " >epoch=12, lrate=0.010, error=242.457 \n",
      " >epoch=13, lrate=0.010, error=242.457 \n",
      " >epoch=14, lrate=0.010, error=242.457 \n",
      " >epoch=15, lrate=0.010, error=242.457 \n",
      " >epoch=16, lrate=0.010, error=242.457 \n",
      " >epoch=17, lrate=0.010, error=242.457 \n",
      " >epoch=18, lrate=0.010, error=242.457 \n",
      " >epoch=19, lrate=0.010, error=242.457 \n",
      " >epoch=20, lrate=0.010, error=242.457 \n",
      " >epoch=21, lrate=0.010, error=242.457 \n",
      " >epoch=22, lrate=0.010, error=242.457 \n",
      " >epoch=23, lrate=0.010, error=242.457 \n",
      " >epoch=24, lrate=0.010, error=242.457 \n",
      " >epoch=25, lrate=0.010, error=242.457 \n",
      " >epoch=26, lrate=0.010, error=242.457 \n",
      " >epoch=27, lrate=0.010, error=242.457 \n",
      " >epoch=28, lrate=0.010, error=242.457 \n",
      " >epoch=29, lrate=0.010, error=242.457 \n",
      " >epoch=30, lrate=0.010, error=242.457 \n",
      " >epoch=31, lrate=0.010, error=242.457 \n",
      " >epoch=32, lrate=0.010, error=242.457 \n",
      " >epoch=33, lrate=0.010, error=242.457 \n",
      " >epoch=34, lrate=0.010, error=242.457 \n",
      " >epoch=35, lrate=0.010, error=242.457 \n",
      " >epoch=36, lrate=0.010, error=242.457 \n",
      " >epoch=37, lrate=0.010, error=242.457 \n",
      " >epoch=38, lrate=0.010, error=242.457 \n",
      " >epoch=39, lrate=0.010, error=242.457 \n",
      " >epoch=40, lrate=0.010, error=242.457 \n",
      " >epoch=41, lrate=0.010, error=242.457 \n",
      " >epoch=42, lrate=0.010, error=242.457 \n",
      " >epoch=43, lrate=0.010, error=242.457 \n",
      " >epoch=44, lrate=0.010, error=242.457 \n",
      " >epoch=45, lrate=0.010, error=242.457 \n",
      " >epoch=46, lrate=0.010, error=242.457 \n",
      " >epoch=47, lrate=0.010, error=242.457 \n",
      " >epoch=48, lrate=0.010, error=242.457 \n",
      " >epoch=49, lrate=0.010, error=242.457 \n",
      " >epoch=50, lrate=0.010, error=242.457 \n",
      " >epoch=51, lrate=0.010, error=242.457 \n",
      " >epoch=52, lrate=0.010, error=242.457 \n",
      " >epoch=53, lrate=0.010, error=242.457 \n",
      " >epoch=54, lrate=0.010, error=242.457 \n",
      " >epoch=55, lrate=0.010, error=242.457 \n",
      " >epoch=56, lrate=0.010, error=242.457 \n",
      " >epoch=57, lrate=0.010, error=242.457 \n",
      " >epoch=58, lrate=0.010, error=242.457 \n",
      " >epoch=59, lrate=0.010, error=242.457 \n",
      " >epoch=60, lrate=0.010, error=242.457 \n",
      " >epoch=61, lrate=0.010, error=242.457 \n",
      " >epoch=62, lrate=0.010, error=242.457 \n",
      " >epoch=63, lrate=0.010, error=242.457 \n",
      " >epoch=64, lrate=0.010, error=242.457 \n",
      " >epoch=65, lrate=0.010, error=242.457 \n",
      " >epoch=66, lrate=0.010, error=242.457 \n",
      " >epoch=67, lrate=0.010, error=242.457 \n",
      " >epoch=68, lrate=0.010, error=242.457 \n",
      " >epoch=69, lrate=0.010, error=242.457 \n",
      " >epoch=70, lrate=0.010, error=242.457 \n",
      " >epoch=71, lrate=0.010, error=242.457 \n",
      " >epoch=72, lrate=0.010, error=242.457 \n",
      " >epoch=73, lrate=0.010, error=242.457 \n",
      " >epoch=74, lrate=0.010, error=242.457 \n",
      " >epoch=75, lrate=0.010, error=242.457 \n",
      " >epoch=76, lrate=0.010, error=242.457 \n",
      " >epoch=77, lrate=0.010, error=242.457 \n",
      " >epoch=78, lrate=0.010, error=242.457 \n",
      " >epoch=79, lrate=0.010, error=242.457 \n",
      " >epoch=80, lrate=0.010, error=242.457 \n",
      " >epoch=81, lrate=0.010, error=242.457 \n",
      " >epoch=82, lrate=0.010, error=242.457 \n",
      " >epoch=83, lrate=0.010, error=242.457 \n",
      " >epoch=84, lrate=0.010, error=242.457 \n",
      " >epoch=85, lrate=0.010, error=242.457 \n",
      " >epoch=86, lrate=0.010, error=242.457 \n",
      " >epoch=87, lrate=0.010, error=242.457 \n",
      " >epoch=88, lrate=0.010, error=242.457 \n",
      " >epoch=89, lrate=0.010, error=242.457 \n",
      " >epoch=90, lrate=0.010, error=242.457 \n",
      " >epoch=91, lrate=0.010, error=242.457 \n",
      " >epoch=92, lrate=0.010, error=242.457 \n",
      " >epoch=93, lrate=0.010, error=242.457 \n",
      " >epoch=94, lrate=0.010, error=242.457 \n",
      " >epoch=95, lrate=0.010, error=242.457 \n",
      " >epoch=96, lrate=0.010, error=242.457 \n",
      " >epoch=97, lrate=0.010, error=242.457 \n",
      " >epoch=98, lrate=0.010, error=242.457 \n",
      " >epoch=99, lrate=0.010, error=242.457 \n",
      " >epoch=100, lrate=0.010, error=242.457 \n",
      " >epoch=101, lrate=0.010, error=242.457 \n",
      " >epoch=102, lrate=0.010, error=242.457 \n",
      " >epoch=103, lrate=0.010, error=242.457 \n",
      " >epoch=104, lrate=0.010, error=242.457 \n",
      " >epoch=105, lrate=0.010, error=242.457 \n",
      " >epoch=106, lrate=0.010, error=242.457 \n",
      " >epoch=107, lrate=0.010, error=242.457 \n",
      " >epoch=108, lrate=0.010, error=242.457 \n",
      " >epoch=109, lrate=0.010, error=242.457 \n",
      " >epoch=110, lrate=0.010, error=242.457 \n",
      " >epoch=111, lrate=0.010, error=242.457 \n",
      " >epoch=112, lrate=0.010, error=242.457 \n",
      " >epoch=113, lrate=0.010, error=242.457 \n",
      " >epoch=114, lrate=0.010, error=242.457 \n",
      " >epoch=115, lrate=0.010, error=242.457 \n",
      " >epoch=116, lrate=0.010, error=242.457 \n",
      " >epoch=117, lrate=0.010, error=242.457 \n",
      " >epoch=118, lrate=0.010, error=242.457 \n",
      " >epoch=119, lrate=0.010, error=242.457 \n",
      " >epoch=120, lrate=0.010, error=242.457 \n",
      " >epoch=121, lrate=0.010, error=242.457 \n",
      " >epoch=122, lrate=0.010, error=242.457 \n",
      " >epoch=123, lrate=0.010, error=242.457 \n",
      " >epoch=124, lrate=0.010, error=242.457 \n",
      " >epoch=125, lrate=0.010, error=242.457 \n",
      " >epoch=126, lrate=0.010, error=242.457 \n",
      " >epoch=127, lrate=0.010, error=242.457 \n",
      " >epoch=128, lrate=0.010, error=242.457 \n",
      " >epoch=129, lrate=0.010, error=242.457 \n",
      " >epoch=130, lrate=0.010, error=242.457 \n",
      " >epoch=131, lrate=0.010, error=242.457 \n",
      " >epoch=132, lrate=0.010, error=242.457 \n",
      " >epoch=133, lrate=0.010, error=242.457 \n",
      " >epoch=134, lrate=0.010, error=242.457 \n",
      " >epoch=135, lrate=0.010, error=242.457 \n",
      " >epoch=136, lrate=0.010, error=242.457 \n",
      " >epoch=137, lrate=0.010, error=242.457 \n",
      " >epoch=138, lrate=0.010, error=242.457 \n",
      " >epoch=139, lrate=0.010, error=242.457 \n",
      " >epoch=140, lrate=0.010, error=242.457 \n",
      " >epoch=141, lrate=0.010, error=242.457 \n",
      " >epoch=142, lrate=0.010, error=242.457 \n",
      " >epoch=143, lrate=0.010, error=242.457 \n",
      " >epoch=144, lrate=0.010, error=242.457 \n",
      " >epoch=145, lrate=0.010, error=242.457 \n",
      " >epoch=146, lrate=0.010, error=242.457 \n",
      " >epoch=147, lrate=0.010, error=242.457 \n",
      " >epoch=148, lrate=0.010, error=242.457 \n",
      " >epoch=149, lrate=0.010, error=242.457 \n",
      " >epoch=150, lrate=0.010, error=242.457 \n",
      " >epoch=151, lrate=0.010, error=242.457 \n",
      " >epoch=152, lrate=0.010, error=242.457 \n",
      " >epoch=153, lrate=0.010, error=242.457 \n",
      " >epoch=154, lrate=0.010, error=242.457 \n",
      " >epoch=155, lrate=0.010, error=242.457 \n",
      " >epoch=156, lrate=0.010, error=242.457 \n",
      " >epoch=157, lrate=0.010, error=242.457 \n",
      " >epoch=158, lrate=0.010, error=242.457 \n",
      " >epoch=159, lrate=0.010, error=242.457 \n",
      " >epoch=160, lrate=0.010, error=242.457 \n",
      " >epoch=161, lrate=0.010, error=242.457 \n",
      " >epoch=162, lrate=0.010, error=242.457 \n",
      " >epoch=163, lrate=0.010, error=242.457 \n",
      " >epoch=164, lrate=0.010, error=242.457 \n",
      " >epoch=165, lrate=0.010, error=242.457 \n",
      " >epoch=166, lrate=0.010, error=242.457 \n",
      " >epoch=167, lrate=0.010, error=242.457 \n",
      " >epoch=168, lrate=0.010, error=242.457 \n",
      " >epoch=169, lrate=0.010, error=242.457 \n",
      " >epoch=170, lrate=0.010, error=242.457 \n",
      " >epoch=171, lrate=0.010, error=242.457 \n",
      " >epoch=172, lrate=0.010, error=242.457 \n",
      " >epoch=173, lrate=0.010, error=242.457 \n",
      " >epoch=174, lrate=0.010, error=242.457 \n",
      " >epoch=175, lrate=0.010, error=242.457 \n",
      " >epoch=176, lrate=0.010, error=242.457 \n",
      " >epoch=177, lrate=0.010, error=242.457 \n",
      " >epoch=178, lrate=0.010, error=242.457 \n",
      " >epoch=179, lrate=0.010, error=242.457 \n",
      " >epoch=180, lrate=0.010, error=242.457 \n",
      " >epoch=181, lrate=0.010, error=242.457 \n",
      " >epoch=182, lrate=0.010, error=242.457 \n",
      " >epoch=183, lrate=0.010, error=242.457 \n",
      " >epoch=184, lrate=0.010, error=242.457 \n",
      " >epoch=185, lrate=0.010, error=242.457 \n",
      " >epoch=186, lrate=0.010, error=242.457 \n",
      " >epoch=187, lrate=0.010, error=242.457 \n",
      " >epoch=188, lrate=0.010, error=242.457 \n",
      " >epoch=189, lrate=0.010, error=242.457 \n",
      " >epoch=190, lrate=0.010, error=242.457 \n",
      " >epoch=191, lrate=0.010, error=242.457 \n",
      " >epoch=192, lrate=0.010, error=242.457 \n",
      " >epoch=193, lrate=0.010, error=242.457 \n",
      " >epoch=194, lrate=0.010, error=242.457 \n",
      " >epoch=195, lrate=0.010, error=242.457 \n",
      " >epoch=196, lrate=0.010, error=242.457 \n",
      " >epoch=197, lrate=0.010, error=242.457 \n",
      " >epoch=198, lrate=0.010, error=242.457 \n",
      " >epoch=199, lrate=0.010, error=242.457 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1728,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9l/pp35pbc538l6tnkr7sm9lzqh0000gn/T/ipykernel_6367/1703690714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Make predictions on the test data u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mY_pred_sgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Calculate the mean squared error and coefficient of determination R2 for the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1728,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add a column of ones to the input data \n",
    "X_test = np.c_[np.ones(len(X_test)), X_test]\n",
    "\n",
    "# Add a column of ones to the input data\n",
    "X_train = np.c_[np.ones(len(X_train)), X_train]\n",
    "\n",
    "coef_sgd = coefficients_sgd(X_train, Y_train, l_rate, n_epoch)\n",
    "\n",
    "# Make predictions on the test data u\n",
    "Y_pred_sgd = X_test.dot(coef_sgd)\n",
    "\n",
    "# Calculate the mean squared error and coefficient of determination R2 for the predictions\n",
    "mse_sgd = mean_squared_error(Y_test, Y_pred_sgd)\n",
    "r2_sgd = r2_score(Y_test, Y_pred_sgd)\n",
    "\n",
    "print(\"Mean squared error using SGD algorithm:\", mse_sgd)\n",
    "print(\"Coefficient of determination R2 using SGD algorithm:\", r2_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc29be",
   "metadata": {},
   "source": [
    "# 2. Problem 2: Create a Perceptron model with an optimal value of hyperparameterα (learning rate of SGD) \n",
    "For this problem, from the zip file you downloaded for Problem 1, use Sonar Dataset which involves\n",
    "the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at\n",
    "different angles. It is a binary (2-class) classiffication problem. The number of observations for each\n",
    "class is not balanced. There are 208 observations with 60 input variables and 1 output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30226afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages to the Jupyter notebook\n",
    "# Implement a Perceptron algorithm with an optimal value of learning rate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "# read and load the csv data file\n",
    "filename = \"/Users/jeremybouhadana/Downloads/myClassDataSet.csv\"\n",
    "df = read_csv(filename)\n",
    "array = df.values\n",
    "# separate array into input and output components\n",
    "X = array [:,:-1]\n",
    "Y = array [:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d1574",
   "metadata": {},
   "source": [
    "## (a) Split your data into train and test portions with ‘test size = 0.3’ and ’random state = 3’ .  Define your learning model to be Perceptron. Use RepeatedStratifiedKFold with ‘n splits=10’, ‘n repeats=5’, and ‘random state=1’ as your model evaluation method. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af4eb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n",
    "\n",
    "# Define the model\n",
    "model = Perceptron()\n",
    "\n",
    "# Define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8a395",
   "metadata": {},
   "source": [
    "## (b) Use GridSearchCV to perform a gird search on the parameter of Perceptron algorithm (learning rate α in SGD), consider values for α as [0.0001, 0.001, 0.01, 0.1]. For your GridSearch, use data only from your training sets (X train, Y train). (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2e70efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate:  0.0001\n",
      "Best accuracy:  0.9997142857142858\n"
     ]
    }
   ],
   "source": [
    "# Define the grid \n",
    "grid = {'alpha': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# Define  grid search\n",
    "search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search and fit the best model\n",
    "best_model = search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best \n",
    "print(\"Best learning rate: \", best_model.best_params_['alpha'])\n",
    "print(\"Best accuracy: \", best_model.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2824e59",
   "metadata": {},
   "source": [
    "## (c) Report the best score and the best value of the parameter in your search. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2923542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.9997142857142858\n",
      "Best learning rate:  0.0001\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: \", best_model.best_score_)\n",
    "print(\"Best learning rate: \", best_model.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e9d47",
   "metadata": {},
   "source": [
    "## (d) Create a Perceptron model which takes as an argument the best value of parameter you found in the previous step, and use this model to make predictions on your test set; Report the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cfe9d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# separate array into input and output components\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n",
    "\n",
    "# Define the model with the best learning rate\n",
    "model = Perceptron(alpha=0.0001)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5dd2e6",
   "metadata": {},
   "source": [
    "# Problem 3: Create a KNN model with an optimal value of hyperparameter K\n",
    "(the number of nearest neighbors) (18 points)\n",
    "Continue working with the dataset from Problem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01bc27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages to the Jupyter notebook\n",
    "# Create a KNN model with the best parameter K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# read and load the csv data file\n",
    "filename = \"/Users/jeremybouhadana/Downloads/myClassDataSet.csv\"\n",
    "df = read_csv(filename)\n",
    "array = df.values\n",
    "# separate array into input and output components\n",
    "X = array [:,:-1]\n",
    "Y = array [:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55742924",
   "metadata": {},
   "source": [
    "## (a) Split the data into train and test sets with ‘test size = 0.3’, and ‘random state = 5’. Create aKNN model with parameter ‘n neighbor’ varying from 1 to 30 (see the code from Lab Session 6). (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7ba2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "test_size = 0.3\n",
    "random_state = 5\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Create a KNN model with parameter n_neighbors varying from 1 to 30\n",
    "k_range = range(1, 31)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    Y_pred = knn.predict(X_test)\n",
    "    scores.append(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194e61d",
   "metadata": {},
   "source": [
    "## (b) Plot the accuracy of the KNN model in terms of the number of nearest neighbor k varying from 1 to 30. Choose and report the best value for k. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f98c8f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of k is 1\n"
     ]
    }
   ],
   "source": [
    "# Plot the accuracy scores for different values of k\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Find the best value of k\n",
    "best_k = np.argmax(scores) + 1\n",
    "print(\"The best value of k is\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e8c2e",
   "metadata": {},
   "source": [
    "# (c) Create a new KNN model with the best values of nearest neighbors that you found in previous step, and perform prediction on your test set. Report the accuracy of the model. (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d091a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the KNN model with k = 1 is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a KNN model with the best value of k\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_best.fit(X_train, Y_train)\n",
    "Y_pred = knn_best.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"The accuracy of the KNN model with k =\", best_k, \"is\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c250a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
